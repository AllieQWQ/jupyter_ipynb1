{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    自行操作 的 豆瓣实训\n",
    "'''\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pymysql\n",
    "import re\n",
    "\n",
    "def get_one_page(url):\n",
    "    header = {\n",
    "        'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0',\n",
    "        'accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'cookie':'ll=\"118318\"; bid=z6gJQ-g_VbA; douban-fav-remind=1; _ga=GA1.1.152072664.1728867251; _ga_RXNMP372GL=GS1.1.1728871022.2.0.1728871022.60.0.0; __utmz=30149280.1729470866.7.6.utmcsr=cn.bing.com|utmccn=(referral)|utmcmd=referral|utmcct=/; __utmv=30149280.28528; ap_v=0,6.0; _pk_id.100001.3ac3=b11897d958dfba39.1742456607.; _pk_ses.100001.3ac3=1; __utma=30149280.148866276.1726189819.1736752951.1742456607.22; __utma=81379588.152072664.1728867251.1742456607.1742456607.1; __utmz=81379588.1742456607.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); dbcl2=\"285281789:czKJVYE1R0c\"; ck=7OEk; frodotk_db=\"3c2301308a87d870b46903919f978950\"; push_noty_num=0; push_doumail_num=0; __utmc=30149280; __utmt=1; __utmt_douban=1; __utmb=30149280.13.10.1742456607; __utmc=81379588; __utmb=81379588.11.10.1742456607'\n",
    "    }\n",
    "\n",
    "    r = requests.get(url,headers = header)\n",
    "    r.encoding = r.apparent_encoding    #设置编码格式\n",
    "    demo = r.text\n",
    "    return demo\n",
    "\n",
    "\n",
    "def parse_one_page(html):\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    Books = soup.select('#content > div.grid-16-8.clearfix > div.article > ul > li')\n",
    "\n",
    "    Book_data = []\n",
    "    for book in Books:\n",
    "        #书名\n",
    "        Book_name = book.select('div.media__body > h2 > a')[0].string\n",
    "        \n",
    "        #作者 但是带出版社？怎么分割\n",
    "        Book_author = book.select('div.media__body > p.subject-abstract.color-gray')[0].string.strip().split(' / ')\n",
    "        \n",
    "        #作者\n",
    "        writer = Book_author[0].strip()\n",
    "\n",
    "        #出版时间\n",
    "        date = '未知'\n",
    "        for i in Book_author:\n",
    "            if re.search(r'\\d{4}-\\d{1,2}(?:-\\d{1,2})?',i):\n",
    "                date = i.strip()\n",
    "                break\n",
    "        \n",
    "\n",
    "        #出版社\n",
    "        company = '未知'\n",
    "        for part in Book_author:\n",
    "            if re.search(r'.*(?:出版|书店|书局|书馆|编选).*',part):\n",
    "                company = part.strip()\n",
    "                break\n",
    "\n",
    "\n",
    "        '''\n",
    "        #出版时间\n",
    "        #date = Book_author[1].strip()   #但是匹配的不准确\n",
    "\n",
    "        #出版社\n",
    "        #company = Book_author[2].strip()    #但是匹配的不准确\n",
    "        '''\n",
    "\n",
    "        #评分\n",
    "        Book_elem = book.select('div.media__body > p.clearfix.w250.subject-rating > span.font-small.fleft')\n",
    "        Book_rank = Book_elem[0].string if Book_elem and Book_elem[0].string else '未评分'\n",
    "        \n",
    "        #评分人数\n",
    "        Book_rankPerson = book.find('span',class_='fleft ml8 color-gray').text.replace('(','').replace(')','')\n",
    "\n",
    "        #图书链接\n",
    "        Book_src = book.select('div.media__body > h2 > a')[0]['href']\n",
    "        \n",
    "\n",
    "        Book_data.append([Book_name,writer,company,date,Book_rank,Book_rankPerson,Book_src])\n",
    "\n",
    "        #print(Book_name,writer,company,date,Book_rank,Book_rankPerson,Book_src)\n",
    "        \n",
    "        \n",
    "    return Book_data\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def write_to_csv(content,path):\n",
    "    with open(path,'a',newline='',encoding='utf-8-sig') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if f.tell() == 0:\n",
    "            writer.writerow(['书名','作者','出版社','出版时间','评分','评分人数','图书链接'])\n",
    "        writer.writerows(content)   #写入数据\n",
    "\n",
    "\n",
    "def write_to_table(content):\n",
    "    #1、创建数据库链接对象connect\n",
    "    db = pymysql.connect(\n",
    "        host='localhost',\n",
    "        port=3306,\n",
    "        user='root',\n",
    "        passwd='1024',\n",
    "        db='douban_book',\n",
    "        charset='utf8'\n",
    "    )\n",
    "\n",
    "    #2、获取游标对象 cursor\n",
    "    cursor = db.cursor()\n",
    "\n",
    "    #3、执行SQL语句（核心步骤：建表、增删改查）\n",
    "    \n",
    "\n",
    "    create_table = \"\"\"\n",
    "    create table if not exists book1(\n",
    "        book_name varchar(100) not null primary key,\n",
    "        book_author varchar(100) not null,\n",
    "        book_company varchar(100) not null,\n",
    "        book_date varchar(100) not null,\n",
    "        book_rank varchar(100) not null,\n",
    "        book_person varchar(100) not null,\n",
    "        book_src varchar(200) not null\n",
    "    )\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table)\n",
    "    db.commit()\n",
    "\n",
    "    \n",
    "\n",
    "    #插入数据\n",
    "    insert_sql = \"\"\"\n",
    "    insert into book1(book_name,book_author,book_company,book_date\n",
    "    ,book_rank,book_person,book_src) values(%s,%s,%s,%s,%s,%s,%s)\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.executemany(insert_sql,content)\n",
    "    db.commit()\n",
    "\n",
    "    db.rollback()\n",
    "\n",
    "    # try:\n",
    "    #     for i in content:\n",
    "    #         cursor.execute(insert_sql,(\n",
    "    #             i[0],i[1],i[2],i[3],i[4],i[5],i[6]\n",
    "    #             )\n",
    "    #         )\n",
    "    #         db.commit()\n",
    "            \n",
    "    # except:\n",
    "    #     db.rollback()\n",
    "        \n",
    "    #4、关闭游标和数据库连接\n",
    "    cursor.close()\n",
    "    db.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for i in range(1,13):\n",
    "        if i == 1:\n",
    "            url = 'https://book.douban.com/latest?subcat=%E5%85%A8%E9%83%A8&p=1'\n",
    "        else:\n",
    "            url = f'https://book.douban.com/latest?subcat=%E5%85%A8%E9%83%A8&p={i}'\n",
    "\n",
    "        html = get_one_page(url)\n",
    "        #print(html)     #尝试打印编码 验证是否成功\n",
    "\n",
    "        content = parse_one_page(html)\n",
    "\n",
    "        #write_to_csv(content,'Book_data.csv')\n",
    "\n",
    "        # #写入数据库\n",
    "        write_to_table(content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 页爬取完成，共 20 条数据\n",
      "第 2 页爬取完成，共 20 条数据\n",
      "第 3 页爬取完成，共 20 条数据\n",
      "第 4 页爬取完成，共 20 条数据\n",
      "第 5 页爬取完成，共 20 条数据\n",
      "第 6 页爬取完成，共 20 条数据\n",
      "第 7 页爬取完成，共 20 条数据\n",
      "第 8 页爬取完成，共 20 条数据\n",
      "第 9 页爬取完成，共 20 条数据\n",
      "第 10 页爬取完成，共 20 条数据\n",
      "第 11 页爬取完成，共 13 条数据\n",
      "第 12 页爬取完成，共 0 条数据\n",
      "全部完成！共爬取 213 条数据\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    自行操作 的 豆瓣实训\n",
    "'''\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pymysql\n",
    "import re\n",
    "\n",
    "def get_one_page(url):\n",
    "    header = {\n",
    "        'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0',\n",
    "        'accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'cookie':'ll=\"118318\"; bid=z6gJQ-g_VbA; douban-fav-remind=1; _ga=GA1.1.152072664.1728867251; _ga_RXNMP372GL=GS1.1.1728871022.2.0.1728871022.60.0.0; __utmz=30149280.1729470866.7.6.utmcsr=cn.bing.com|utmccn=(referral)|utmcmd=referral|utmcct=/; __utmv=30149280.28528; ap_v=0,6.0; _pk_id.100001.3ac3=b11897d958dfba39.1742456607.; _pk_ses.100001.3ac3=1; __utma=30149280.148866276.1726189819.1736752951.1742456607.22; __utma=81379588.152072664.1728867251.1742456607.1742456607.1; __utmz=81379588.1742456607.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); dbcl2=\"285281789:czKJVYE1R0c\"; ck=7OEk; frodotk_db=\"3c2301308a87d870b46903919f978950\"; push_noty_num=0; push_doumail_num=0; __utmc=30149280; __utmt=1; __utmt_douban=1; __utmb=30149280.13.10.1742456607; __utmc=81379588; __utmb=81379588.11.10.1742456607'\n",
    "    }\n",
    "\n",
    "    r = requests.get(url,headers = header)\n",
    "    r.encoding = r.apparent_encoding    #设置编码格式\n",
    "    demo = r.text\n",
    "    return demo\n",
    "\n",
    "\n",
    "def parse_one_page(html):\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    Books = soup.select('#content > div.grid-16-8.clearfix > div.article > ul > li')\n",
    "\n",
    "    Book_data = []\n",
    "    for book in Books:\n",
    "        #书名\n",
    "        Book_name = book.select('div.media__body > h2 > a')[0].string\n",
    "        \n",
    "        #作者 但是带出版社？怎么分割\n",
    "        Book_author = book.select('div.media__body > p.subject-abstract.color-gray')[0].string.strip().split(' / ')\n",
    "        \n",
    "        #作者\n",
    "        writer = Book_author[0].strip()\n",
    "\n",
    "        #出版时间\n",
    "        date = '未知'\n",
    "        for i in Book_author:\n",
    "            if re.search(r'\\d{4}-\\d{1,2}(?:-\\d{1,2})?',i):\n",
    "                date = i.strip()\n",
    "                break\n",
    "        \n",
    "\n",
    "        #出版社\n",
    "        company = '未知'\n",
    "        for part in Book_author:\n",
    "            if re.search(r'.*(?:出版|书店|书局|书馆|编选).*',part):\n",
    "                company = part.strip()\n",
    "                break\n",
    "\n",
    "\n",
    "        '''\n",
    "        #出版时间\n",
    "        #date = Book_author[1].strip()   #但是匹配的不准确\n",
    "\n",
    "        #出版社\n",
    "        #company = Book_author[2].strip()    #但是匹配的不准确\n",
    "        '''\n",
    "\n",
    "        #评分\n",
    "        Book_elem = book.select('div.media__body > p.clearfix.w250.subject-rating > span.font-small.fleft')\n",
    "        Book_rank = Book_elem[0].string if Book_elem and Book_elem[0].string else '未评分'\n",
    "        \n",
    "        #评分人数\n",
    "        Book_rankPerson = book.find('span',class_='fleft ml8 color-gray').text.replace('(','').replace(')','')\n",
    "\n",
    "        #图书链接\n",
    "        Book_src = book.select('div.media__body > h2 > a')[0]['href']\n",
    "        \n",
    "\n",
    "        Book_data.append([Book_name,writer,company,date,Book_rank,Book_rankPerson,Book_src])\n",
    "\n",
    "        #print(Book_name,writer,company,date,Book_rank,Book_rankPerson,Book_src)\n",
    "        \n",
    "        \n",
    "    return Book_data\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def write_to_csv(content,path):\n",
    "    with open(path,'a',newline='',encoding='utf-8-sig') as f:\n",
    "        writer = csv.writer(f)\n",
    "        if f.tell() == 0:\n",
    "            writer.writerow(['书名','作者','出版社','出版时间','评分','评分人数','图书链接'])\n",
    "        writer.writerows(content)   #写入数据\n",
    "\n",
    "\n",
    "def write_to_table(content):\n",
    "    #1、创建数据库链接对象connect\n",
    "    db = pymysql.connect(\n",
    "        host='localhost',\n",
    "        port=3306,\n",
    "        user='root',\n",
    "        passwd='1024',\n",
    "        db='douban_book',\n",
    "        charset='utf8'\n",
    "    )\n",
    "\n",
    "    #2、获取游标对象 cursor\n",
    "    cursor = db.cursor()\n",
    "\n",
    "    #3、执行SQL语句（核心步骤：建表、增删改查）\n",
    "    '''\n",
    "\n",
    "    create_table = \"\"\"\n",
    "    create table if not exists book(\n",
    "        book_name varchar(100) not null primary key,\n",
    "        book_author varchar(100) not null,\n",
    "        book_company varchar(100) not null,\n",
    "        book_date varchar(100) not null,\n",
    "        book_rank varchar(100) not null,\n",
    "        book_person varchar(100) not null,\n",
    "        book_src varchar(200) not null\n",
    "    )\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table)\n",
    "    db.commit()\n",
    "\n",
    "    '''\n",
    "\n",
    "    #插入数据\n",
    "    insert_sql = \"\"\"\n",
    "    insert into book(book_name,book_author,book_company,book_date\n",
    "    ,book_rank,book_person,book_src) values(%s,%s,%s,%s,%s,%s,%s)\n",
    "    \"\"\"\n",
    "\n",
    "    # cursor.executemany(insert_sql,content)\n",
    "    # db.commit()\n",
    "\n",
    "    # db.rollback()\n",
    "\n",
    "    try:\n",
    "        for i in content:\n",
    "            cursor.execute(insert_sql,(\n",
    "                i[0],i[1],i[2],i[3],i[4],i[5],i[6]\n",
    "                )\n",
    "            )\n",
    "            db.commit()\n",
    "            \n",
    "    except:\n",
    "        db.rollback()\n",
    "        \n",
    "    #4、关闭游标和数据库连接\n",
    "    cursor.close()\n",
    "    db.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    all_books = []  # 新增：用于收集所有页面的数据\n",
    "    \n",
    "    for i in range(1, 13):\n",
    "        if i == 1:\n",
    "            url = 'https://book.douban.com/latest?subcat=%E5%85%A8%E9%83%A8&p=1'\n",
    "        else:\n",
    "            url = f'https://book.douban.com/latest?subcat=%E5%85%A8%E9%83%A8&p={i}'\n",
    "\n",
    "        html = get_one_page(url)\n",
    "        page_books = parse_one_page(html)  # 获取当前页数据\n",
    "        all_books.extend(page_books)  # 添加到总列表\n",
    "\n",
    "        print(f\"第 {i} 页爬取完成，共 {len(page_books)} 条数据\")\n",
    "    \n",
    "    # 所有页面爬取完成后，一次性写入\n",
    "    write_to_csv(all_books, 'Book_data.csv')  # 如果需要写入CSV\n",
    "    write_to_table(all_books)  # 写入数据库\n",
    "    \n",
    "    print(f\"全部完成！共爬取 {len(all_books)} 条数据\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
